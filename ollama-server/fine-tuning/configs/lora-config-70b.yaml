# LoRA Fine-Tuning Configuration for Llama 3.3 70B
# Hardware: 48GB+ GPU VRAM (A100, H100, or multi-GPU setup)
# Training time: ~8-12 hours for 100-200 emails

# Model configuration
base_model: "meta-llama/Llama-3.3-70B-Instruct"
model_max_length: 2048

# Data paths
train_data: "training-data/train_alpaca.jsonl"
val_data: "training-data/val_alpaca.jsonl"
data_format: "alpaca"  # alpaca or chat

# LoRA configuration
lora_r: 32                # Higher rank for larger model
lora_alpha: 64            # Alpha scaling (2x rank)
lora_dropout: 0.05
lora_target_modules: null # Use default

# Quantization (required for 70B on consumer hardware)
use_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"

# Training hyperparameters
output_dir: "./output/llama-3.3-70b-email"
num_train_epochs: 2       # Fewer epochs for larger model
per_device_train_batch_size: 1  # Smaller batch for memory
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16  # Effective batch size = 16
learning_rate: 1.0e-4     # Lower LR for larger model
max_grad_norm: 0.3
warmup_ratio: 0.05
lr_scheduler_type: "cosine"

# Logging and checkpointing
logging_steps: 10
save_steps: 50
eval_steps: 50
save_total_limit: 2

# Optimization
fp16: false
bf16: true
gradient_checkpointing: true
group_by_length: true
