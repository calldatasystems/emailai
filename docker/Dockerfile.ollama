FROM nvidia/cuda:12.1.0-base-ubuntu22.04

# Ollama server for EmailAI production deployment
# Install dependencies
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Expose Ollama API port
EXPOSE 11434

# Set environment variable for Ollama to bind to all interfaces
ENV OLLAMA_HOST=0.0.0.0:11434

# Create startup script that works with Vast.ai's SSH runtime
RUN echo '#!/bin/bash\n\
# Start Ollama in background\n\
nohup ollama serve > /var/log/ollama.log 2>&1 &\n\
# Wait for Ollama to be ready\n\
for i in {1..30}; do\n\
  if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then\n\
    echo "Ollama is ready"\n\
    break\n\
  fi\n\
  echo "Waiting for Ollama to start... ($i/30)"\n\
  sleep 2\n\
done\n\
# Keep container running and tail logs\n\
tail -f /var/log/ollama.log\n\
' > /start-ollama.sh && chmod +x /start-ollama.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:11434/api/tags || exit 1

# Use ENTRYPOINT so it runs even when Vast.ai overrides CMD
ENTRYPOINT ["/start-ollama.sh"]
